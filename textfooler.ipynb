{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Part 1: Preprocessing and Exploration of the AG_News Dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "\n",
    "print(\"Cuda availablity is:\", cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the URLs of the dataset files on GitHub\n",
    "train_url = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n",
    "test_url = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\"\n",
    "\n",
    "# Load the dataset using pandas\n",
    "train_df = pd.read_csv(train_url, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "test_df = pd.read_csv(test_url, header=None, names=[\"label\", \"title\", \"description\"])\n",
    "\n",
    "print(\"Train dataset shape:\", train_df.shape)\n",
    "print(\"Test dataset shape:\", test_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the title and description columns in both the train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['title'] + \" \" + train_df['description']\n",
    "test_df['text'] = test_df['title'] + \" \" + test_df['description']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the text using the Hugging Face Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Choose a pre-trained model architecture (e.g., BERT)\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Instantiate a tokenizer based on a pre-trained model (e.g., BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the text in the train and test dataframes\n",
    "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the labels into numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 1 from the label values to make them zero-based (i.e., 0 to 3 instead of 1 to 4)\n",
    "train_labels = train_df['label'].values - 1\n",
    "test_labels = test_df['label'].values - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PyTorch DataLoader objects for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_text_and_label(self, idx):\n",
    "        text = self.encodings.tokenizer.decode(self.encodings['input_ids'][idx])\n",
    "        label = self.labels[idx]\n",
    "        return text, label\n",
    "    \n",
    "# Randomly sample a subset of the original dataset for training\n",
    "train_df_sample = train_df.sample(frac=0.001, random_state=42)\n",
    "\n",
    "# Tokenize the text in the sampled train dataframe and the test dataframe\n",
    "train_encodings = tokenizer(train_df_sample['text'].tolist(), truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Convert the labels into numerical format using the sampled train dataframe\n",
    "train_labels = train_df_sample['label'].values - 1\n",
    "test_labels = test_df['label'].values - 1\n",
    "\n",
    "# Create dataset objects for the sampled train data and test data\n",
    "train_dataset = AGNewsDataset(train_encodings, train_labels)\n",
    "test_dataset = AGNewsDataset(test_encodings, test_labels)\n",
    "\n",
    "# Create DataLoader objects for the sampled train data and test data\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Part 2: Train and validate the text classification model using pyTorch</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# from transformers import AdamW\n",
    "# AdamW optimizer from the transformers library is deprecated\n",
    "\n",
    "# Instantiate the model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nWhen initializing the BertForSequenceClassification model from the pre-trained BERT model, some weights are not used and some are newly initialized, as expected\")\n",
    "print(\"\\nThis model should be fine-tuned on a downstream task for better performance\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model on the preprocessed dataset for several epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# Train the model for the desired number of epochs\n",
    "num_epochs = 3\n",
    "if torch.cuda.is_available():\n",
    "    num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the trained model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model's performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    return true_labels, pred_labels\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "true_labels, pred_labels = evaluate(model, test_loader, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision = precision_score(true_labels, pred_labels, average='weighted')\n",
    "recall = recall_score(true_labels, pred_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Part 3: Implement the TextFooler-based Attack</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import TextFooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the test set for generating adversarial examples\n",
    "subset_test_df = test_df.sample(n=100, random_state=42)\n",
    "subset_test_encodings = tokenizer(subset_test_df['text'].tolist(), truncation=True, padding=True, max_length=256)\n",
    "subset_test_labels = subset_test_df['label'].values - 1\n",
    "subset_test_dataset = AGNewsDataset(subset_test_encodings, subset_test_labels)\n",
    "subset_test_loader = DataLoader(subset_test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom dataset class for TextAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextDataset:\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        for item in dataset:\n",
    "            text = self.tokenizer.decode(item['input_ids'])\n",
    "            label = item['labels'].item()\n",
    "            self.texts.append(text)\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a TextAttack dataset from the subset of the test set using the custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_test_dataset_custom = SimpleTextDataset(subset_test_dataset, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Model for TextFooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textattack\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "import pickle\n",
    "\n",
    "# Wrap the trained model for use with TextAttack\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# Instantiate the TextFooler attack\n",
    "attack = TextFoolerJin2019.build(model_wrapper)\n",
    "\n",
    "# Generate adversarial examples\n",
    "adversarial_examples = []\n",
    "for i in range(len(subset_test_dataset_custom)):\n",
    "    original_text, ground_truth_label = subset_test_dataset_custom[i]\n",
    "    attack_result = attack.attack(original_text, ground_truth_label)\n",
    "    adversarial_examples.append(attack_result)\n",
    "\n",
    "# Save the generated adversarial examples for future analysis\n",
    "with open(\"adversarial_examples.pkl\", \"wb\") as f:\n",
    "    pickle.dump(adversarial_examples, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <font color='blue'>Part 4: Evaluate the Impact on the Model's Performance </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the adversarial examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"adversarial_examples.pkl\", \"rb\") as f:\n",
    "    adversarial_examples = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function to compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, tokenizer, dataset):\n",
    "    total_count = len(dataset)\n",
    "    correct_count = 0\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for text, true_label in dataset:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}  # Move inputs to the same device as the model\n",
    "        outputs = model(**inputs)\n",
    "        predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "        if predicted_label == true_label:\n",
    "            correct_count += 1\n",
    "\n",
    "    return correct_count / total_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on the original test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_accuracy = compute_accuracy(model, tokenizer, subset_test_dataset_custom)\n",
    "print(\"Original Accuracy: {:.2f}%\".format(original_accuracy * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataset with the adversarial examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.attack_results import SuccessfulAttackResult\n",
    "\n",
    "successful_attacks = [example for example in adversarial_examples if isinstance(example, SuccessfulAttackResult)]\n",
    "adversarial_dataset = [(example.perturbed_text(), example.original_result.ground_truth_output) for example in successful_attacks]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on the adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_accuracy = compute_accuracy(model, tokenizer, adversarial_dataset)\n",
    "print(\"Adversarial Accuracy: {:.2f}%\".format(adversarial_accuracy * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study: Visualizing the difference between original and adversarial text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 5  # Number of examples to display\n",
    "\n",
    "print(\"Case Study: Visualizing the difference between original and adversarial text\\n\")\n",
    "\n",
    "for i, attack_result in enumerate(successful_attacks[:num_examples]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Original Text:\")\n",
    "    print(attack_result.original_text())\n",
    "    print(\"\\nAdversarial Text:\")\n",
    "    print(attack_result.perturbed_text())\n",
    "    print(\"\\nGround Truth Label:\", attack_result.original_result.ground_truth_output)\n",
    "    print(\"Predicted Label (Original):\", attack_result.original_result.output)\n",
    "    print(\"Predicted Label (Adversarial):\", attack_result.perturbed_result.output)\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <font color='blue'>Part 5: Adverserial Training </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate adversarial examples using the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset, range(10))  # Use a subset of 1000 samples\n",
    "train_subset_custom = SimpleTextDataset(train_subset, tokenizer)\n",
    "\n",
    "adversarial_train_examples = []\n",
    "\n",
    "for original_text, ground_truth_label in tqdm(train_subset_custom, desc=\"Generating adversarial examples\"):\n",
    "    attack_result = attack.attack(original_text, ground_truth_label)\n",
    "    if isinstance(attack_result, SuccessfulAttackResult):\n",
    "        adversarial_train_examples.append((attack_result.perturbed_text(), ground_truth_label))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mix the original training dataset with the generated adversarial examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_train_dataset = [(text, label) for text, label in train_subset_custom] + adversarial_train_examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model on the mixed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Use a DataLoader to handle batching of the mixed dataset\n",
    "mixed_train_dataloader = DataLoader(mixed_train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1\n",
    "if torch.cuda.is_available():\n",
    "    num_epochs = 50\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in tqdm(mixed_train_dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        texts, labels = batch\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "        \n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model's performance on the test dataset and adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Evaluate on the original test dataset\n",
    "test_accuracy = compute_accuracy(model, tokenizer, subset_test_dataset_custom)\n",
    "print(f\"Accuracy on the test dataset: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on the adversarial examples\n",
    "adversarial_accuracy = compute_accuracy(model, tokenizer, adversarial_dataset)\n",
    "print(f\"Accuracy on the adversarial examples: {adversarial_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
